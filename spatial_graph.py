# -*- coding: utf-8 -*-
"""spatial_graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zVirwGKN9-L8-V9nk6CKZfmPrLi5NUJt
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

import pandas as pd
df = pd.read_csv('augmented.csv')
# Get unique values in 'Application_Type' column
unique_values = df['Application_Type'].unique().tolist()
print("Unique values:", unique_values)

# Create a mapping dictionary
encoding_dict = {value: idx for idx, value in enumerate(unique_values)}

# Apply the mapping to the 'Application_Type' column
df['Application_Type'] = df['Application_Type'].map(encoding_dict)

print("Encoding Dictionary:", encoding_dict)
df

# Convert Mbps to Kbps
df['Required_Bandwidth'] = df['Required_Bandwidth'] * 1000
df['Allocated_Bandwidth'] = df['Allocated_Bandwidth'] * 1000

print(df)

df.info()

sns.countplot(x=df['Allocated_Bandwidth'])

sns.countplot(x=df['Resource_Allocation'])

!pip install torch-geometric

import networkx as nx
import torch
from torch_geometric.utils import from_networkx
import pandas as pd
from scipy.spatial import KDTree

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

# Load data
data = pd.read_csv("/content/drive/MyDrive/1_GNN/augmented.csv")

# Preprocess data
data['User_ID'] = data['User_ID'].astype(str)

# Extract spatial coordinates
coords = data[['posx', 'posy']].values

# Create a KDTree for efficient nearest-neighbor search
kdtree = KDTree(coords)

# Find pairs of points within the threshold distance
threshold_distance = 15
pairs = kdtree.query_pairs(threshold_distance)

# Create a graph
G = nx.Graph()

# Add nodes with attributes
for _, row in data.iterrows():
    G.add_node(row['User_ID'],
               pos=(row['posx'], row['posy']),
               application=row['Application_Type'],
               signal_strength=row['Signal_Strength'],
               latency=row['Latency'],
               required_bandwidth=row['Required_Bandwidth'],
               allocated_bandwidth=row['Allocated_Bandwidth'],
               resource_allocation=row['Resource_Allocation'])

# Add edges based on proximity
for i, j in pairs:
    G.add_edge(data['User_ID'][i], data['User_ID'][j])

# Convert to PyTorch Geometric data
graph_data = from_networkx(G)

# Include spatial information in the node features
node_features = data[['posx', 'posy', 'Signal_Strength', 'Latency', 'Required_Bandwidth', 'Allocated_Bandwidth', 'Resource_Allocation']].values
graph_data.x = torch.tensor(node_features, dtype=torch.float)

# Move graph data to GPU
graph_data = graph_data.to(device)

# Confirm that the data is on GPU
print(graph_data)

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels=7, out_channels=16)  # 7 input features
        self.conv2 = GCNConv(in_channels=16, out_channels=32)
        self.fc = torch.nn.Linear(32, 1)  # Predicting a single value (resource allocation)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.fc(x)
        return x.squeeze()

from sklearn.model_selection import train_test_split

# Extract target variable (resource allocation)
y = data['Resource_Allocation'].values

# Split data into training and testing sets
train_idx, test_idx = train_test_split(range(len(data)), test_size=0.2, random_state=42)

train_mask = torch.zeros(len(data), dtype=torch.bool)
train_mask[train_idx] = True

test_mask = torch.zeros(len(data), dtype=torch.bool)
test_mask[test_idx] = True

# Add masks and target variable to graph data
graph_data.train_mask = train_mask
graph_data.test_mask = test_mask
graph_data.y = torch.tensor(y, dtype=torch.float).to(device)

model = GCN().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.MSELoss()

def train():
    model.train()
    optimizer.zero_grad()
    out = model(graph_data)
    loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

def test():
    model.eval()
    with torch.no_grad():
        out = model(graph_data)
        loss = criterion(out[graph_data.test_mask], graph_data.y[graph_data.test_mask])
    return loss.item()

for epoch in range(500):
    train_loss = train()
    test_loss = test()
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')

model.eval()
with torch.no_grad():
    out = model(graph_data)
    predicted = out[graph_data.test_mask].cpu().numpy()
    actual = graph_data.y[graph_data.test_mask].cpu().numpy()

print('Predicted Resource Allocation:', predicted)
print('Actual Resource Allocation:', actual)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score



# Calculate metrics
mae = mean_absolute_error(actual, predicted)
mse = mean_squared_error(actual, predicted)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((np.array(actual) - np.array(predicted)) / np.array(actual))) * 100
r_squared = r2_score(actual, predicted)

# Print metrics
print(f'MAE: {mae}')
print(f'MSE: {mse}')
print(f'RMSE: {rmse}')
print(f'MAPE: {mape}%')
print(f'R-squared: {r_squared}')

# Since F1 Score is usually for classification, it may not be appropriate here,
# but if you still want to calculate it, you could convert the regression output to binary classification.
# Here's an example based on a threshold:
threshold = np.mean(actual)  # For example, use the mean of actual values as a threshold
predicted_binary = [1 if x >= threshold else 0 for x in predicted]
actual_binary = [1 if x >= threshold else 0 for x in actual]

f1 = f1_score(actual_binary, predicted_binary)
print(f'F1 Score: {f1}')

import matplotlib.pyplot as plt

# Updated metrics data
metrics = {
    'MAE': 12.875458717346191,
    'MSE': 232.85733032226562,
    'RMSE': 15.259663581848145,
    'MAPE': 18.245428800582886,
    'R-squared': -0.13488003210327304,
    'F1 Score': 0.5314998401023345
}

# Create bar plot
plt.figure(figsize=(10, 6))
plt.bar(metrics.keys(), metrics.values(), color='skyblue')

# Add labels and title
plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Evaluation Metrics for Spatial- Graph')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--')

# Add value labels on top of the bars
for i, v in enumerate(metrics.values()):
    plt.text(i, v + (max(metrics.values()) * 0.05), f'{v:.2f}', ha='center', va='bottom')
# Show plot
plt.show()

# Save the model's state dictionary
torch.save(model.state_dict(), '/content/drive/MyDrive/1_GNN/gnn_model.pth')

import torch
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels=7, out_channels=16)  # 7 input features
        self.conv2 = GCNConv(in_channels=16, out_channels=32)
        self.fc = torch.nn.Linear(32, 1)  # Predicting a single value (resource allocation)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.fc(x)
        return x.squeeze()

# Instantiate the model
model = GCN()

# Load the saved model state dictionary
model.load_state_dict(torch.load('/content/drive/MyDrive/1_GNN/gnn_model.pth'))

# Set the model to evaluation mode
model.eval()

# Use the model for prediction
with torch.no_grad():
    # Assuming graph_data is already prepared as before
    out = model(graph_data)
    predicted = out[graph_data.test_mask].cpu().numpy()
    actual = graph_data.y[graph_data.test_mask].cpu().numpy()

print('Predicted Resource Allocation:', predicted)
print('Actual Resource Allocation:', actual)
# Compute metrics
mse = mean_squared_error(actual, predicted)
rmse = np.sqrt(mse)
mae = mean_absolute_error(actual, predicted)
r2 = r2_score(actual, predicted)

# Print metrics
print('Mean Squared Error (MSE):', mse)
print('Root Mean Squared Error (RMSE):', rmse)
print('Mean Absolute Error (MAE):', mae)
print('R-squared (R2):', r2)

class Node:
    def __init__(self, node_id, application_type, signal_strength, latency, required_bandwidth_kbps, allocated_bandwidth_kbps):
        self.node_id = node_id
        self.application_type = application_type
        self.signal_strength = signal_strength
        self.latency = latency
        self.required_bandwidth_kbps = required_bandwidth_kbps
        self.allocated_bandwidth_kbps = allocated_bandwidth_kbps

    def __repr__(self):
        return (f"Node({self.node_id}, {self.application_type}, {self.signal_strength:.2f}, "
                f"{self.latency:.2f} ms, {self.required_bandwidth_kbps:.2f} kbps, "
                f"{self.allocated_bandwidth_kbps:.2f} kbps)")

import random

def create_node(node_id):
    application_types = ['Video Streaming', 'Web Browsing', 'VoIP', 'Gaming']
    return Node(
        node_id=node_id,
        application_type=random.choice(application_types),
        signal_strength=random.uniform(0, 1),  # Assuming signal strength between 0 and 1
        latency=random.uniform(1, 100),  # Latency in milliseconds
        required_bandwidth_kbps=random.uniform(100, 10000),  # Bandwidth in kbps
        allocated_bandwidth_kbps=random.uniform(100, 10000)  # Bandwidth in kbps
    )

graph_data = create_node(1)
print(graph_data)

!pip install torch-geometric

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import networkx as nx
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

# Load the dataset
file_path = 'augmented.csv'
data = pd.read_csv(file_path)

# Preprocess the data (example, you may need additional preprocessing)
def preprocess(data):
    data['Signal_Strength'] = data['Signal_Strength'].astype(float)
    data['Latency'] = data['Latency'].astype(float)
    return data

data = preprocess(data)

# Define the GNN model
class GNN(nn.Module):
    def __init__(self):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(3, 16)
        self.conv2 = GCNConv(16, 32)
        self.fc1 = nn.Linear(32, 16)
        self.fc2 = nn.Linear(16, 1)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        x = torch.relu(x)
        x = torch.mean(x, dim=0)
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

# Define the RL agent
class RLAgent:
    def __init__(self, gnn, lr=0.001):
        self.gnn = gnn
        self.optimizer = optim.Adam(gnn.parameters(), lr=lr)
        self.criterion = nn.MSELoss()

    def train(self, data, target):
        self.optimizer.zero_grad()
        output = self.gnn(data)
        loss = self.criterion(output, target)
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def predict(self, data):
        with torch.no_grad():
            return self.gnn(data).item()

# Initialize the network environment
network_capacity = 100  # Total available bandwidth
allocated_bandwidth = 0
latency_threshold = 50  # Latency threshold for performance evaluation

# Initialize performance metrics
total_users = len(data)
successful_allocations = 0
failed_allocations = 0
total_latency = 0
total_required_bandwidth = 0
total_allocated_bandwidth = 0

# Create GNN model and RL agent
gnn = GNN()
rl_agent = RLAgent(gnn)

# Prepare data for GNN (example, you may need to adjust based on your graph structure)
edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)  # Example edges
x = torch.tensor([[1, 1, 1] for _ in range(total_users)], dtype=torch.float)  # Example node features

# Create PyTorch Geometric data object
data_gnn = Data(x=x, edge_index=edge_index)

# Simulate user activity
for index, row in data.iterrows():
    user_id = row['User_ID']
    application_type = row['Application_Type']
    required_bandwidth = row['Required_Bandwidth']
    signal_strength = row['Signal_Strength']
    latency = row['Latency']
    posx = row['posx']
    posy = row['posy']

    total_required_bandwidth += required_bandwidth

    # Predict optimal allocation using GNN and RL
    allocation = rl_agent.predict(data_gnn)

    # Resource allocation
    if allocated_bandwidth + allocation <= network_capacity:
        allocated_bandwidth += allocation
        total_allocated_bandwidth += allocation
        successful_allocations += 1
    else:
        failed_allocations += 1

    # Measure performance
    total_latency += latency

    # Update RL agent (example, adjust based on your training logic)
    target = torch.tensor([required_bandwidth], dtype=torch.float)
    loss = rl_agent.train(data_gnn, target)

# Calculate performance metrics
average_latency = total_latency / total_users
resource_allocation_efficiency = (total_allocated_bandwidth / total_required_bandwidth) * 100
success_rate = (successful_allocations / total_users) * 100
failure_rate = (failed_allocations / total_users) * 100

# Print performance metrics
print(f"Total Users: {total_users}")
print(f"Successful Allocations: {successful_allocations}")
print(f"Failed Allocations: {failed_allocations}")
print(f"Average Latency: {average_latency:.2f} ms")
print(f"Resource Allocation Efficiency: {resource_allocation_efficiency:.2f} %")
print(f"Success Rate: {success_rate:.2f} %")
print(f"Failure Rate: {failure_rate:.2f} %")

# Visualize performance metrics
labels = ['Success', 'Failure']
sizes = [successful_allocations, failed_allocations]
colors = ['#4CAF50', '#FF6347']
explode = (0.1, 0)  # explode the first slice

plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Allocation Success vs Failure')

plt.subplot(1, 2, 2)
plt.bar(['Average Latency', 'Resource Allocation Efficiency'], [average_latency, resource_allocation_efficiency], color=['#1E90FF', '#32CD32'])
plt.title('Performance Metrics')
plt.ylabel('Value')

plt.tight_layout()
plt.show()

model.eval()
with torch.no_grad():
    out = model(graph_data)
    predicted = out[graph_data.test_mask].cpu().numpy()
    actual = graph_data.y[graph_data.test_mask].cpu().numpy()

print('Predicted Resource Allocation:', predicted)
print('Actual Resource Allocation:', actual)

"""required  bandwidth x
allocated bandwidth y
"""